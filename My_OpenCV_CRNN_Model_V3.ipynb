{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "db47d1f32d5fb28c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-31T18:41:46.902070Z",
     "start_time": "2025-08-31T18:41:45.158728Z"
    }
   },
   "source": [
    "import os, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from datasets import Dataset as HFDataset, Features, Value\n",
    "from datasets import Image as HFImage\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config",
   "id": "e3fe51139ea66500"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T18:42:43.603085Z",
     "start_time": "2025-08-31T18:42:43.591547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED         = 2025\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# (A) paths (EDIT THESE TWO)\n",
    "BASE_DIR     = r\"C:\\Arvin\\icbt\\assignments\\Final Project\\mjsynth_500k\"\n",
    "MANIFEST     = rf\"{BASE_DIR}\\manifest.tsv\"\n",
    "\n",
    "# (B) subset sizes (edit any time)\n",
    "N_TRAIN      = 10_000\n",
    "N_TEST       = 1_000\n",
    "\n",
    "# (C) training knobs\n",
    "IMG_H, IMG_W = 32, 128           # widen to 256 if your words are long (then retrain & re-export)\n",
    "BATCH_SIZE   = 128\n",
    "EPOCHS       = 5                  # raise to 10-20 for better accuracy\n",
    "LR           = 1e-3\n",
    "NUM_WORKERS  = 0                  # Windows-safe\n",
    "PIN_MEMORY   = torch.cuda.is_available()\n",
    "\n",
    "# (D) charset (must include every char in your labels; SPACE included)\n",
    "DIGITS = \"0123456789\"\n",
    "UPPER  = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "LOWER  = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "SYMS   = \"-_:.,!?@#&()+*/=%$'\\\"[]{}<>\\\\|\"\n",
    "CHARS  = DIGITS + UPPER + LOWER + SYMS + \" \"\n",
    "BLANK_IDX = 0\n",
    "char2idx = {c:i+1 for i,c in enumerate(CHARS)}  # 1..N for chars; 0 is CTC blank\n",
    "idx2char = {i+1:c for i,c in enumerate(CHARS)}\n",
    "NCLASS   = len(CHARS) + 1\n",
    "\n",
    "# (E) artifacts\n",
    "ARTIFACTS = Path(BASE_DIR) / \"artifacts\"\n",
    "ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_BEST = ARTIFACTS / f\"crnn_nc{NCLASS}_best.pt\"\n",
    "CKPT_LAST = ARTIFACTS / f\"crnn_nc{NCLASS}_last.pt\"\n",
    "ONNX_PATH = ARTIFACTS / f\"crnn_nc{NCLASS}.onnx\"\n",
    "with open(ARTIFACTS/\"charset.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"chars\": CHARS}, f, ensure_ascii=False)"
   ],
   "id": "d71ee4129d6ccdea",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build a HF dataset from ONE manifest (skip header if present)",
   "id": "f46298cd2363b935"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T18:43:32.827493Z",
     "start_time": "2025-08-31T18:43:26.104674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    MANIFEST, sep=\"\\t\", header=None, names=[\"image\",\"label\"],\n",
    "    dtype={\"image\": str, \"label\": str}, engine=\"python\"\n",
    ")\n",
    "# drop a header row if someone wrote \"image\\tlabel\"\n",
    "df = df[df[\"image\"].str.lower() != \"image\"].reset_index(drop=True)\n",
    "\n",
    "# prepend absolute path and filter missing files\n",
    "df[\"image\"] = df[\"image\"].apply(lambda rel: os.path.join(BASE_DIR, rel))\n",
    "df = df[df[\"image\"].apply(os.path.exists)].reset_index(drop=True)\n",
    "assert len(df) > 0, \"No valid rows after filtering missing files.\"\n",
    "\n",
    "# make HF dataset that yields PIL.Image automatically\n",
    "features = Features({\"image\": HFImage(), \"label\": Value(\"string\")})\n",
    "hf_full  = HFDataset.from_pandas(df, features=features)\n",
    "\n",
    "# shuffle once, then select fixed counts\n",
    "hf_full  = hf_full.shuffle(seed=SEED)\n",
    "take_tr  = min(N_TRAIN, len(hf_full))\n",
    "take_te  = min(N_TEST, max(0, len(hf_full)-take_tr))\n",
    "train_ds_hf = hf_full.select(range(take_tr))\n",
    "val_ds_hf   = hf_full.select(range(take_tr, take_tr+take_te))\n",
    "\n",
    "print(f\"Loaded: train={len(train_ds_hf)} | val={len(val_ds_hf)}\")\n"
   ],
   "id": "671b040650a1ee53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: train=10000 | val=1000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Preprocess + label encode + wrapper + collate",
   "id": "b77ad35bc77e7283"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T18:44:03.177019Z",
     "start_time": "2025-08-31T18:44:03.140729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_gray_keep_ratio(pil_img, target_h=IMG_H, target_w=IMG_W):\n",
    "    if pil_img.mode != \"L\":\n",
    "        pil_img = pil_img.convert(\"L\")\n",
    "    g = np.array(pil_img, dtype=np.uint8)\n",
    "    h, w = g.shape[:2]\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    nw, nh = max(1, int(w*scale)), max(1, int(h*scale))\n",
    "    r = cv2.resize(g, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    canvas = np.full((target_h, target_w), 255, np.uint8)\n",
    "    y0 = (target_h - nh)//2\n",
    "    x0 = 0  # left align\n",
    "    canvas[y0:y0+nh, x0:x0+nw] = r\n",
    "    return torch.from_numpy(canvas).unsqueeze(0).float()/255.0  # [1,H,W]\n",
    "\n",
    "def encode_label(text):\n",
    "    arr = [char2idx[c] for c in text if c in char2idx]\n",
    "    if len(arr) == 0:\n",
    "        arr = [char2idx['A']]\n",
    "    return torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "class HFWrapper(Dataset):\n",
    "    def __init__(self, hf_ds):\n",
    "        self.hf = hf_ds\n",
    "    def __len__(self):\n",
    "        return len(self.hf)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.hf[idx]         # {\"image\": PIL.Image, \"label\": str}\n",
    "        img_t = preprocess_gray_keep_ratio(ex[\"image\"])\n",
    "        lab_t = encode_label(ex[\"label\"])\n",
    "        return img_t, lab_t, ex[\"label\"]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, labs, raws = zip(*batch)         # always (tensor, tensor, str)\n",
    "    labs = list(labs); raws = list(raws)\n",
    "    for i, l in enumerate(labs):\n",
    "        if len(l) == 0:\n",
    "            labs[i] = torch.tensor([char2idx['A']], dtype=torch.long)\n",
    "            raws[i] = \"A\"\n",
    "    imgs = torch.stack(imgs)               # [B,1,H,W]\n",
    "    label_lengths = torch.tensor([len(l) for l in labs], dtype=torch.long)\n",
    "    labels_concat = torch.cat(labs)\n",
    "    return imgs, labels_concat, label_lengths, raws\n",
    "\n",
    "train_dl = DataLoader(HFWrapper(train_ds_hf), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "val_dl   = DataLoader(HFWrapper(val_ds_hf), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)"
   ],
   "id": "c73cd3a5ddcc5ea9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CRNN model (CNN -> BiLSTM -> Linear)",
   "id": "1df66e4d729e8122"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T18:44:38.312768Z",
     "start_time": "2025-08-31T18:44:37.224521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, nclass):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2,2),                         # 32x128 -> 16x64\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2,2),                         # 16x64 -> 8x32\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2,1),(2,1)),                 # 8x32  -> 4x32\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2,1),(2,1)),                 # 4x32  -> 2x32\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=(2,1), stride=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(512, 256, num_layers=2, bidirectional=True, batch_first=False)\n",
    "        self.fc  = nn.Linear(512, nclass)\n",
    "\n",
    "    def forward(self, x):               # x: [B,1,32,128]\n",
    "        f = self.cnn(x)                 # [B,512,1,32]\n",
    "        f = f.squeeze(2)                # [B,512,32]\n",
    "        f = f.permute(2,0,1)            # [T=32, B, 512]\n",
    "        r, _ = self.rnn(f)              # [T, B, 512]\n",
    "        y = self.fc(r)                  # [T, B, nclass]\n",
    "        return y\n",
    "\n",
    "model = CRNN(NCLASS).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "ctc_loss  = nn.CTCLoss(blank=BLANK_IDX, zero_infinity=True)\n",
    "\n",
    "def ctc_greedy_decode(logits):          # logits: [T,B,C]\n",
    "    pred = logits.argmax(2).detach().cpu().numpy()\n",
    "    T, B = pred.shape\n",
    "    outs = []\n",
    "    for b in range(B):\n",
    "        seq, prev, chars = pred[:, b], BLANK_IDX, []\n",
    "        for s in seq:\n",
    "            if s != prev and s != BLANK_IDX:\n",
    "                chars.append(idx2char.get(int(s), \"\"))\n",
    "            prev = s\n",
    "        outs.append(\"\".join(chars))\n",
    "    return outs"
   ],
   "id": "dfd706f5a19ede",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train loop (safe checkpointing)",
   "id": "eddb38ee2a0a9c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-31T18:45:05.155071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total, exact = 0.0, 0, 0\n",
    "    for imgs, labels_concat, label_lengths, raw in tqdm(dl, leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels_concat = labels_concat.to(DEVICE)\n",
    "\n",
    "        logits = model(imgs)                        # [T,B,C]\n",
    "        log_probs = F.log_softmax(logits, dim=2)\n",
    "        T, B, C = log_probs.shape\n",
    "        input_lengths = torch.full((B,), T, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        loss = ctc_loss(log_probs, labels_concat, input_lengths, label_lengths.to(DEVICE))\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total += B\n",
    "\n",
    "        preds = ctc_greedy_decode(logits.detach())\n",
    "        for p, gt in zip(preds, raw):\n",
    "            exact += (p == gt)\n",
    "\n",
    "    return total_loss / max(1, len(dl)), exact / max(1, total)\n",
    "\n",
    "best_val = -1.0\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(train_dl, True)\n",
    "    vl_loss, vl_acc = run_epoch(val_dl, False)\n",
    "    print(f\"[Epoch {ep}/{EPOCHS}] train_loss={tr_loss:.4f} acc={tr_acc*100:.2f}% | val_loss={vl_loss:.4f} acc={vl_acc*100:.2f}%\")\n",
    "    torch.save(model.state_dict(), CKPT_LAST)\n",
    "    if vl_acc >= best_val:\n",
    "        best_val = vl_acc\n",
    "        torch.save(model.state_dict(), CKPT_BEST)\n",
    "\n",
    "print(\"Saved:\", CKPT_BEST, \"|\", CKPT_LAST)"
   ],
   "id": "3e090580f5f78ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "459921c68a514d1ab2c8609200429e68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export ONNX",
   "id": "b7af1aeb19074759"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import onnx, onnxruntime as ort\n",
    "\n",
    "model.load_state_dict(torch.load(CKPT_BEST, map_location=DEVICE))\n",
    "model.eval()\n",
    "dummy = torch.randn(1,1,IMG_H,IMG_W, device=DEVICE)\n",
    "torch.onnx.export(\n",
    "    model, dummy, str(ONNX_PATH),\n",
    "    input_names=[\"input\"], output_names=[\"logits\"],\n",
    "    opset_version=14\n",
    ")\n",
    "print(\"ONNX saved to:\", ONNX_PATH)"
   ],
   "id": "628e2ac85cb04888"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
